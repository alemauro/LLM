# Comparador de LLM - Dual Response

AplicaciÃ³n web para comparar respuestas de diferentes modelos de lenguaje (OpenAI y Anthropic) en tiempo real.

## CaracterÃ­sticas

- ğŸ¤– ComparaciÃ³n simultÃ¡nea de respuestas de OpenAI y Anthropic
- ğŸ›ï¸ SelecciÃ³n dinÃ¡mica de modelos para cada proveedor
- ğŸŒ¡ï¸ Control de temperatura ajustable (creatividad vs determinismo)
- ğŸ“Š Contador de prompts generados persistente
- ğŸ¨ Interfaz moderna y responsiva en espaÃ±ol
- ğŸ³ Docker-ready para deployment en Coolify

## Modelos Disponibles

### OpenAI
- gpt-4o-mini-2024-07-18
- gpt-3.5-turbo-0125

### Anthropic
- claude-3-5-haiku-latest
- claude-3-5-sonnet-20240620

## InstalaciÃ³n

### Requisitos Previos
- Node.js 18 o superior
- npm o yarn
- API Keys de OpenAI y Anthropic

### ConfiguraciÃ³n

1. Clona el repositorio:
```bash
cd dual-llm-app
```

2. Instala las dependencias:
```bash
npm install
```

3. Configura las variables de entorno:
```bash
cp .env.example .env
```

Edita el archivo `.env` y agrega tus API keys:
```env
OPENAI_API_KEY=tu-api-key-de-openai
ANTHROPIC_API_KEY=tu-api-key-de-anthropic
```

### Desarrollo

Ejecuta la aplicaciÃ³n en modo desarrollo:
```bash
npm run dev
```

La aplicaciÃ³n estarÃ¡ disponible en:
- Frontend: http://localhost:5173
- Backend: http://localhost:3000

### ProducciÃ³n

1. Construye la aplicaciÃ³n:
```bash
npm run build
```

2. Ejecuta el servidor:
```bash
npm start
```

## Docker

### ConstrucciÃ³n de la imagen:
```bash
docker build -t dual-llm-app .
```

### EjecuciÃ³n del contenedor:
```bash
docker run -p 3000:3000 \
  -e OPENAI_API_KEY=tu-api-key \
  -e ANTHROPIC_API_KEY=tu-api-key \
  -v dual-llm-data:/app/data \
  dual-llm-app
```

## Deployment en Coolify

1. Conecta tu repositorio con Coolify
2. Configura las siguientes variables de entorno:
   - `OPENAI_API_KEY`
   - `ANTHROPIC_API_KEY`
3. Agrega un volumen persistente:
   - Source: `dual-llm-data`
   - Destination: `/app/data`
4. Deploy automÃ¡tico con cada push a main

## API Endpoints

- `POST /api/llm/generate` - Genera respuestas de ambos LLM
- `GET /api/llm/models` - Obtiene modelos disponibles
- `GET /api/statistics` - Obtiene estadÃ­sticas de uso
- `GET /api/health` - Health check del servidor

## Estructura del Proyecto

```
dual-llm-app/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ backend/        # Servidor Express + TypeScript
â”‚   â””â”€â”€ frontend/       # React + Vite
â”œâ”€â”€ public/             # Assets estÃ¡ticos
â”œâ”€â”€ docs/               # DocumentaciÃ³n
â”œâ”€â”€ data/               # Datos persistentes
â””â”€â”€ dist/               # Build de producciÃ³n
```

## Scripts Disponibles

- `npm run dev` - Desarrollo (frontend + backend)
- `npm run build` - Construir para producciÃ³n
- `npm start` - Ejecutar en producciÃ³n
- `npm run lint` - Linting del cÃ³digo
- `npm run typecheck` - VerificaciÃ³n de tipos

## Autor

**Alejandro Mauro** - Agosto 2025

## Licencia

MIT